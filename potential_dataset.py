# potential_dataset.py
import torch
from torch.utils.data import Dataset
import numpy as np
from pathlib import Path
import logging
import random

class PotentialFieldDataset(Dataset):
    """
    PyTorch Dataset for loading preprocessed agent-centric potential field data.

    Reads .npz files generated by the preprocessing script. Each item corresponds
    to a single agent's observation and target potential field patch.
    """
    def __init__(self, data_dir, obs_radius, file_limit=None):
        """
        Args:
            data_dir (str or Path): Directory containing the .npz files.
            obs_radius (int): Expected observation radius (for verification).
            file_limit (int, optional): Limit the number of NPZ files to load (for faster testing). Defaults to None.
        """
        self.data_dir = Path(data_dir)
        if not self.data_dir.is_dir():
            raise FileNotFoundError(f"Data directory not found: {self.data_dir}")

        self.obs_radius = obs_radius
        self.obs_window_shape = (obs_radius * 2 + 1, obs_radius * 2 + 1)

        self.samples = [] # List of tuples: (path_to_npz, agent_index_in_file)
        npz_files = sorted(list(self.data_dir.glob('*.npz')))

        if file_limit is not None:
            logging.warning(f"Limiting dataset to {file_limit} NPZ files.")
            npz_files = npz_files[:file_limit]

        if not npz_files:
             raise RuntimeError(f"No .npz files found in {self.data_dir}")


        logging.info(f"Scanning {len(npz_files)} NPZ files in {self.data_dir}...")
        total_agents = 0
        skipped_files = 0
        for npz_path in npz_files:
            try:
                # Load only metadata first if possible, or load num_agents minimally
                # Using allow_pickle=True for agent_metadata potentially stored as object array
                with np.load(npz_path, allow_pickle=True) as data:
                    # Verify observation shape if possible from metadata
                    file_obs_shape = tuple(data.get('obs_window_shape', ()))
                    if file_obs_shape and file_obs_shape != self.obs_window_shape:
                         logging.warning(f"Skipping {npz_path}: Observation window shape mismatch. Expected {self.obs_window_shape}, File has {file_obs_shape}")
                         skipped_files += 1
                         continue

                    # Get number of agents from one of the arrays' first dimension
                    if 'input_tensors' in data:
                         num_agents_in_file = data['input_tensors'].shape[0]
                    elif 'agent_metadata' in data: # Fallback to metadata length
                         num_agents_in_file = len(data['agent_metadata'])
                    else:
                         logging.warning(f"Skipping {npz_path}: Cannot determine number of agents (missing 'input_tensors' or 'agent_metadata').")
                         skipped_files += 1
                         continue


                    if num_agents_in_file <= 0:
                         logging.warning(f"Skipping {npz_path}: Found 0 agents.")
                         skipped_files += 1
                         continue

                    for agent_idx in range(num_agents_in_file):
                        self.samples.append((npz_path, agent_idx))
                    total_agents += num_agents_in_file

            except Exception as e:
                logging.error(f"Error scanning file {npz_path}: {e}. Skipping this file.")
                skipped_files += 1

        if not self.samples:
             raise RuntimeError(f"No valid agent samples found after scanning {len(npz_files)} files in {self.data_dir}.")

        logging.info(f"Dataset initialized: Found {len(self.samples)} agent samples across {len(npz_files) - skipped_files} files (skipped {skipped_files} files).")

    def __len__(self):
        """Returns the total number of agent samples."""
        return len(self.samples)

    def __getitem__(self, idx):
        """
        Loads and returns a single agent's data sample.

        Args:
            idx (int): The index of the sample.

        Returns:
            tuple: (input_tensor, target_potential) as torch.Tensors.
        """
        npz_path, agent_idx = self.samples[idx]

        try:
            # Load the specific file - consider caching if I/O is slow and memory allows
            with np.load(npz_path, allow_pickle=True) as data:
                input_tensor_np = data['input_tensors'][agent_idx]
                target_potential_np = data['target_potentials'][agent_idx]

                # Verify shapes again (optional, but good sanity check)
                if input_tensor_np.shape[1:] != self.obs_window_shape or \
                   target_potential_np.shape != self.obs_window_shape:
                    raise ValueError(f"Data shape mismatch in {npz_path} for agent {agent_idx}. "
                                     f"Input: {input_tensor_np.shape}, Target: {target_potential_np.shape}, "
                                     f"Expected Obs Shape: {self.obs_window_shape}")

                # Convert to PyTorch tensors
                # Input shape: (Channels, H, W)
                # Target shape: (H, W) -> Add channel dim: (1, H, W) for loss calculation
                input_tensor = torch.from_numpy(input_tensor_np.astype(np.float32))
                target_potential = torch.from_numpy(target_potential_np.astype(np.float32)).unsqueeze(0) # Add channel dimension

                return input_tensor, target_potential

        except Exception as e:
            logging.error(f"Error loading sample idx {idx} from file {npz_path}, agent {agent_idx}: {e}")
            # Return a dummy sample or raise error? Returning dummy might hide issues.
            # Let's try returning None and handle it in the dataloader collate_fn or training loop
            # Or simpler: just raise the error to stop training if data is corrupt
            raise e


# --- Example Usage (for verification) ---
if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO) # Ensure logging is configured

    # --- Configuration ---
    # ADJUST THIS PATH to your preprocessed data directory
    PREPROCESSED_DATA_DIR = "prepared_agent_centric_dataset_v3" # Example path
    OBS_RADIUS_CONFIG = 5 # Must match the radius used during preprocessing
    BATCH_SIZE_TEST = 16

    try:
        # Create Dataset instance
        dataset = PotentialFieldDataset(data_dir=PREPROCESSED_DATA_DIR, obs_radius=OBS_RADIUS_CONFIG, file_limit=5) # Limit files for quick test

        # Create DataLoader instance
        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=BATCH_SIZE_TEST,
            shuffle=True, # Shuffle for testing purposes here
            num_workers=0 # Use 0 for basic testing, increase for performance
        )

        logging.info(f"Dataset length: {len(dataset)}")

        # Iterate through a few batches to check data loading
        logging.info("Fetching a few batches...")
        num_batches_to_check = 3
        for i, batch in enumerate(dataloader):
            if i >= num_batches_to_check:
                break
            inputs, targets = batch
            logging.info(f"Batch {i+1}:")
            logging.info(f"  Input shape: {inputs.shape}") # Expected: (BatchSize, 4, ObsH, ObsW)
            logging.info(f"  Target shape: {targets.shape}") # Expected: (BatchSize, 1, ObsH, ObsW)

            # Basic check for tensor types and shapes
            assert inputs.dtype == torch.float32
            assert targets.dtype == torch.float32
            assert inputs.dim() == 4 and targets.dim() == 4
            assert inputs.shape[0] <= BATCH_SIZE_TEST
            assert inputs.shape[1] == 4 # Assuming 4 input channels
            assert targets.shape[1] == 1 # Target has 1 channel
            assert inputs.shape[2:] == targets.shape[2:] == (OBS_RADIUS_CONFIG*2+1, OBS_RADIUS_CONFIG*2+1)

        logging.info("DataLoader test completed successfully.")

    except FileNotFoundError as fnf_err:
         logging.error(f"Test failed: {fnf_err}")
         logging.error(f"Please ensure the directory '{PREPROCESSED_DATA_DIR}' exists and contains .npz files.")
    except RuntimeError as rt_err:
         logging.error(f"Test failed: {rt_err}")
         logging.error("This might indicate no valid .npz files were found or issues during scanning.")
    except Exception as err:
         logging.error(f"An unexpected error occurred during dataset/dataloader test: {err}", exc_info=True)